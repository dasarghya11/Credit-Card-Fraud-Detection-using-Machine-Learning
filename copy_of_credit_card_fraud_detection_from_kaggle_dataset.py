# -*- coding: utf-8 -*-
"""Copy of Credit Card Fraud Detection from Kaggle dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jUF3iZEEa_fw2GAJGldZl-MhNKI1gP_P
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('/content/creditcard.csv')

pd.options.display.max_columns = None

"""1. Display Top 5 Rows of The Dataset"""

data.head()

"""2. Check Last 5 Rows of The Dataset"""

data.tail()

"""3. Find Shape of Our Dataset (Number of Rows And Number of Columns)"""

data.shape

print("Number of Rows",data.shape[0])
print("Number of Columns",data.shape[1])

"""4. Get Information About Our Dataset Like Total Number Rows, Total Number of Columns, Datatypes of Each Column And Memory Requirement"""

data.info()

"""5. Check Null Values In The Dataset"""

data.isnull().sum()

"""Feature Scaling"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
data['Amount']=sc.fit_transform(pd.DataFrame(data['Amount']))

data.head()

data = data.drop(['Time'],axis=1)

data.shape

data.duplicated().sum()

"""Let's remove Duplicated Values"""

data = data.drop_duplicates()

data.shape

(19898 - 19332)

data['Class'].value_counts()

import seaborn as sns

sns.countplot(data['Class'])

"""7. Store Feature Matrix In X And Response (Target) In Vector y"""

X = data.drop('Class',axis=1)
y = data['Class']

"""8. Splitting The Dataset Into The Training Set And Test Set"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)

"""9. Handling Imbalanced Dataset       

Undersampling

---


"""

normal = data[data['Class']==0]
fraud = data[data['Class']==1]

normal.shape

fraud.shape

normal_sample=normal.sample(n=85)

new_data = pd.concat([normal_sample,fraud],ignore_index=True)

new_data['Class'].value_counts()

new_data.head()

X = new_data.drop('Class',axis=1)
y = new_data['Class']

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=42)

"""10. Logistic Regression"""

from sklearn.linear_model import LogisticRegression
log = LogisticRegression()
log.fit(X_train,y_train)

y_pred1 = log.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,y_pred1)

from sklearn.metrics import precision_score,recall_score,f1_score

recall_score(y_test,y_pred1)

f1_score(y_test,y_pred1)

"""11. Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train,y_train)

y_pred2 = dt.predict(X_test)

accuracy_score(y_test,y_pred2)

precision_score(y_test,y_pred2)

recall_score(y_test,y_pred2)

f1_score(y_test,y_pred2)

"""12. Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
rf.fit(X_train,y_train)

y_pred3 = rf.predict(X_test)

accuracy_score(y_test,y_pred3)

recall_score(y_test,y_pred3)

f1_score(y_test,y_pred3)

final_data = pd.DataFrame({'Models':['LR','DT','RF'],
              "ACC":[accuracy_score(y_test,y_pred1)*100,
                     accuracy_score(y_test,y_pred2)*100,
                     accuracy_score(y_test,y_pred3)*100
                    ]})

final_data

sns.barplot(final_data['Models'],final_data['ACC'])

"""Oversampling"""

X = data.drop('Class',axis=1)
y = data['Class']

X.shape

y.shape

y_res.value_counts()

"""10. Logistic Regression

"""

log = LogisticRegression()
log.fit(X_train,y_train)

y_pred1 = log.predict(X_test)

accuracy_score(y_test,y_pred1)

precision_score(y_test,y_pred1)

recall_score(y_test,y_pred1)

f1_score(y_test,y_pred1)

"""11. Decision Tree Classifier"""

dt=DecisionTreeClassifier()
dt.fit(X_train,y_train)

y_pred2 = dt.predict(X_test)

precision_score(y_test,y_pred2)

recall_score(y_test,y_pred2)

f1_score(y_test,y_pred2)

"""**12. Random Forest Classifier**"""

rf = RandomForestClassifier()
rf.fit(X_train,y_train)

y_pred3 = rf.predict(X_test)

accuracy_score(y_test,y_pred3)

precision_score(y_test,y_pred3)

recall_score(y_test,y_pred3)

f1_score(y_test,y_pred3)

final_data = pd.DataFrame({'Models':['LR','DT','RF'],
              "ACC":[accuracy_score(y_test,y_pred1)*100,
                     accuracy_score(y_test,y_pred2)*100,
                     accuracy_score(y_test,y_pred3)*100
                    ]})

final_data

sns.barplot(final_data['Models'],final_data['ACC'])

"""SAVE THE MODEL."""

rf1 = RandomForestClassifier()
rf1.fit(X_res,y_res)

import joblib

joblib.dump(rf1,"credit_card_model")

model = joblib.load("credit_card_model")

pred = model.predict([[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]])

if pred == 0:
    print("Normal Transcation")
else:
    print("Fraudulent Transcation")

